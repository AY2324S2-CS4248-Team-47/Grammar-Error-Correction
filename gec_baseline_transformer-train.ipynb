{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIbHmmqKHjgp"
   },
   "source": [
    "# GEC Baseline - Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXQf4UjC-OZB"
   },
   "source": [
    "## Downloading required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpiJtPN8-SEK"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFN-wq0n57FV"
   },
   "outputs": [],
   "source": [
    "!pip install -U accelerate\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJWUB-uh-SRS"
   },
   "source": [
    "## Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzekRIbR-WJu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import wandb\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoConfig\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf75eOFn1Ob2"
   },
   "source": [
    "## WandB Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arZiSbU41UNN"
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_0O0b0TTBxO"
   },
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRgrio4NNSiP"
   },
   "source": [
    "### Dataset structure\n",
    "The fields we care about in the dataset are structured as below\n",
    "- **text**: gramatically incorrect text (input to model)\n",
    "- **edits**: Each edit is a span `[start:end]` where the original text should be replaced by `edits.text`\n",
    "  - **start**: start indexes of each edit as a list of integers\n",
    "  - **end**: end indexes of each edit as a list of integers\n",
    "  - **text**: the text content of each edit as a list of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wyntdYGNsNW"
   },
   "source": [
    "### Tokenize + Create labels\n",
    "\n",
    "The inputs and labels for the transformer are as below\n",
    "\n",
    "- **Inputs** = grammatically incorrect text\n",
    "- **Labels** = corrected text\n",
    "\n",
    "1. I tokenize and divide each input text into smaller chunks of length at most `max_len`. A `stride` is used to create overlaps between windows (so that the model learns grammar errors that occur across boundaries of chunks). These are now the inputs to the model.\n",
    "2. Then, I compute the appropriate corrected text for each \"chunk\" and tokenize these. These become the labels for the model.\n",
    "\n",
    "- At the moment, I'm replacing corrections that are `None` with `tokenizer.unk_token`. An example where this occurs is printed at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVxBLBpIN1j4"
   },
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"wi_locness\", \"wi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOw1IKQDN3q8"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oe2Kmu-rNsNW"
   },
   "outputs": [],
   "source": [
    "## these are hyperparams that can be tuned\n",
    "max_len = 256\n",
    "stride = max_len // 2  # half length of prev window included in next window\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # how to handle prefix? not handled currently (but only relevant for T5 finetuning)\n",
    "    inputs = tokenizer(\n",
    "        text=examples[\"text\"],\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        stride=stride,\n",
    "    )\n",
    "\n",
    "    labels_out = []\n",
    "    overflow_to_sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        example_idx = overflow_to_sample_mapping[i]\n",
    "\n",
    "        start_idx = offset_mapping[i][0][0]\n",
    "        end_idx = offset_mapping[i][-2][1]  # last token is <eos>, so we care about second last tok offset\n",
    "        edits = examples[\"edits\"][example_idx]\n",
    "        corrected_text = examples[\"text\"][example_idx][start_idx:end_idx]\n",
    "\n",
    "        for start, end, correction in reversed(\n",
    "            list(zip(edits[\"start\"], edits[\"end\"], edits[\"text\"]))\n",
    "        ):\n",
    "\n",
    "            if start < start_idx or end > end_idx:\n",
    "                continue\n",
    "            start_offset = start - start_idx  # >= 0\n",
    "            end_offset = end - start_idx\n",
    "            if correction == None:\n",
    "                correction = tokenizer.unk_token\n",
    "            corrected_text = (\n",
    "                corrected_text[:start_offset] + correction + corrected_text[end_offset:]\n",
    "            )\n",
    "\n",
    "        labels_out.append(corrected_text)\n",
    "\n",
    "    labels_out = tokenizer(labels_out, max_length=512, truncation=True)\n",
    "    inputs[\"labels\"] = labels_out[\"input_ids\"]\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woe-J-HcOD75"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-IHGAHoUMw1"
   },
   "source": [
    "### Batching\n",
    "`DataCollatorForSeq2Seq` will automatically pad the input texts to the same size and batch them for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vn2ydvEzNjyG"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4vcRbeLzaxr"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShW3x2seDjZg"
   },
   "source": [
    "### Creating the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JP6Qejsgzrnq"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDervJLY113J"
   },
   "source": [
    "### Set WandB params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcGtdlFP14DA"
   },
   "outputs": [],
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"] = \"gec-baseline-transformer\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZi-9AjvOSaK"
   },
   "source": [
    "### Set training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBYmQ9UO2voq"
   },
   "outputs": [],
   "source": [
    "output_dir = 'baseline-transformer'\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\tauto_find_batch_size=True,\n",
    "    num_train_epochs=100,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"]\n",
    ")\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SO_mNFokzi-f"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter notebook --ServerApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1Z9nrR9OQPY"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9GevJ4a8dPS"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ./baseline-transformer\n",
    "# !rm -rf ./wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sentence):\n",
    "    input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I is go to the park.\"\n",
    "inference(sentence)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
