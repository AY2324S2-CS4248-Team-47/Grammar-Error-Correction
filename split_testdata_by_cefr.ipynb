{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "U_9g7enGu3Z9"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "raw_datasets = load_dataset(\"wi_locness\", 'wi')\n",
        "print(raw_datasets)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "model_checkpoint = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "TpNQvINyejJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f297c9c7-d982-46f6-81bf-600b273ad0f2"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'userid', 'cefr', 'text', 'edits'],\n",
            "        num_rows: 3000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'userid', 'cefr', 'text', 'edits'],\n",
            "        num_rows: 300\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = examples['text']\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "    labels_out = []\n",
        "    offset_mapping = model_inputs.pop(\"offset_mapping\")\n",
        "    for i in range(len(model_inputs[\"input_ids\"])):\n",
        "        example_idx = i\n",
        "\n",
        "        start_idx = offset_mapping[i][0][0]\n",
        "        end_idx = offset_mapping[i][-2][1]  # last token is <eos>, so we care about second last tok offset\n",
        "\n",
        "        edits = examples[\"edits\"][example_idx]\n",
        "\n",
        "        corrected_text = inputs[example_idx][start_idx:end_idx]\n",
        "\n",
        "        for start, end, correction in reversed(\n",
        "            list(zip(edits[\"start\"], edits[\"end\"], edits[\"text\"]))\n",
        "        ):\n",
        "            if start < start_idx or end > end_idx:\n",
        "                continue\n",
        "            start_offset = start - start_idx  # >= 0\n",
        "            end_offset = end - start_idx\n",
        "            if correction == None:\n",
        "                correction = tokenizer.unk_token\n",
        "            corrected_text = (\n",
        "                corrected_text[:start_offset] + correction + corrected_text[end_offset:]\n",
        "            )\n",
        "\n",
        "        labels_out.append(corrected_text)\n",
        "\n",
        "    labels_out = tokenizer(labels_out, max_length=512, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels_out[\"input_ids\"]\n",
        "\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "22XEFbHIen6l"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True\n",
        ")"
      ],
      "metadata": {
        "id": "BPFRQXDBeqmj"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test split of 90%-10%\n",
        "dataset_dict = tokenized_datasets[\"train\"].train_test_split(test_size=0.1, seed=0)\n",
        "print(dataset_dict)\n",
        "\n",
        "tokenized_datasets[\"train\"] = dataset_dict[\"train\"]\n",
        "tokenized_datasets[\"test\"] = dataset_dict[\"test\"]\n",
        "print(tokenized_datasets)\n",
        "\n",
        "X_train = tokenized_datasets[\"train\"][\"input_ids\"]\n",
        "Y_train = tokenized_datasets[\"train\"][\"labels\"]\n",
        "cefr_train = tokenized_datasets[\"train\"][\"cefr\"]\n",
        "\n",
        "X_test = tokenized_datasets[\"test\"][\"input_ids\"]\n",
        "Y_test = tokenized_datasets[\"test\"][\"labels\"]\n",
        "cefr_test = tokenized_datasets[\"test\"][\"cefr\"]\n",
        "\n",
        "print(len(X_test), len(Y_test), len(cefr_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF5dX06DvinW",
        "outputId": "7e5d2ed0-28cb-4fdc-df1b-52715d6a531b"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'userid', 'cefr', 'text', 'edits', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 2700\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'userid', 'cefr', 'text', 'edits', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 300\n",
            "    })\n",
            "})\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'userid', 'cefr', 'text', 'edits', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 2700\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'userid', 'cefr', 'text', 'edits', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 300\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'userid', 'cefr', 'text', 'edits', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 300\n",
            "    })\n",
            "})\n",
            "300 300 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cefr_train_date = Counter(cefr_train)\n",
        "cefr_test_data = Counter(cefr_test)\n",
        "\n",
        "keys = set(cefr_train_data.keys()) | set(cefr_test_data.keys())\n",
        "combined_cefr = {}\n",
        "for key in keys:\n",
        "  value1 = cefr_train_data.get(key, 0)\n",
        "  value2 = cefr_test_data.get(key, 0)\n",
        "  combined_cefr[key] = [value1, value2]\n",
        "combined_cefr = dict(sorted(combined_cefr.items()))\n",
        "\n",
        "\n",
        "print(\"{:<10} {:<10} {:<10}\".format('CEFR', 'TRAIN', 'TEST'))\n",
        "for key, value in combined_cefr.items():\n",
        "    cefr = key\n",
        "    tr, te = value\n",
        "    print(\"{:<10} {:<10} {:<10}\".format(cefr, tr, te))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNKWkSDJwNSh",
        "outputId": "1a66fbfc-e4f5-4ac9-bfeb-3c0de42c5878"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CEFR       TRAIN      TEST      \n",
            "A1.i       169        15        \n",
            "A1.ii      226        30        \n",
            "A2.i       233        39        \n",
            "A2.ii      314        38        \n",
            "B1.i       197        36        \n",
            "B1.ii      216        32        \n",
            "B2.i       190        27        \n",
            "B2.ii      119        17        \n",
            "C1.i       157        17        \n",
            "C1.ii      175        22        \n",
            "C2+        5          1         \n",
            "C2.i       112        16        \n",
            "C2.ii      74         10        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data by CEFR\n",
        "Explanation of the cefr classifications can be found [here](https://www.cambridgeenglish.org/exams-and-tests/cefr/)\n",
        "*   Run the first cell to get tokenized X_test and Y_test\n",
        "*   Run the second cell to get the decoded (original sentences) of X_test, Y_test\n"
      ],
      "metadata": {
        "id": "ZZDbB8om1xJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test = {}\n",
        "for k in sorted(cefr_test_data.keys()):\n",
        "  tokenized_test[k] = {\"X\":[], \"Y\":[]}\n",
        "\n",
        "for cefr, src, ref in zip(cefr_test, X_test, Y_test):\n",
        "  tokenized_test[cefr][\"X\"].append(src)\n",
        "  tokenized_test[cefr][\"Y\"].append(ref)\n",
        "\n",
        "print(tokenized_test['A1.i']['X'][0])\n",
        "print(tokenized_test['A1.i']['Y'][0])\n",
        "\n",
        "# To get the X_test and Y_test for cefr A1.i\n",
        "# X_test_A1 = tokenized_test['A1.i']['X']\n",
        "# Y_test_A1 = tokenized_test['A1.i']['Y']"
      ],
      "metadata": {
        "id": "B0wqi10-yIKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_test = {}\n",
        "for k in sorted(cefr_test_data.keys()):\n",
        "  decoded_test[k] = {\"X\":[], \"Y\":[]}\n",
        "\n",
        "for cefr, src, ref in zip(cefr_test, X_test, Y_test):\n",
        "  decoded_test[cefr][\"X\"].append(tokenizer.decode(src))\n",
        "  decoded_test[cefr][\"Y\"].append(tokenizer.decode(ref))\n",
        "\n",
        "print(decoded_test['A1.i']['X'][0])\n",
        "print(decoded_test['A1.i']['Y'][0])\n",
        "\n",
        "# To get the X_test and Y_test for cefr A1.i\n",
        "# X_test_decoded_A1 = decoded_test['A1.i']['X']\n",
        "# Y_test_decoded_A1 = decoded_test['A1.i']['Y']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh8L3X8nymZV",
        "outputId": "8301b0e3-ad03-442e-bd60-20d1c36c367e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Gareth, I can't go to a barbecue next Sunday because I'm going to Seville with my parents and I'm coming back so late. I think that we can meet on Saturday evening at the cafe. What do you think about it? Love, Alicia</s>\n",
            "Dear Gareth, I can't go to the barbecue next Sunday because I'm going to Seville with my parents and I'm coming back so late. I think that we can meet on Saturday evening at the cafe. What do you think about it? Love, Alicia</s>\n",
            "dict_keys(['A1.i', 'A1.ii', 'A2.i', 'A2.ii', 'B1.i', 'B1.ii', 'B2.i', 'B2.ii', 'C1.i', 'C1.ii', 'C2.i', 'C2.ii'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into Beginner (all A* cefr) , Intermediate (all B* cefr), Advanced (All C* cefr)\n",
        "*   Run the first cell to get tokenized X_test and Y_test\n",
        "*   Run the second cell to get the decoded (original sentences) of X_test, Y_test\n",
        "\n",
        "To get all X_test and Y_test for beginner texts\n",
        "```\n",
        "X_test = tokenized_test['A']['X']\n",
        "Y_test = tokenized_test['A']['Y']\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Bshe9pAH1u25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test = {}\n",
        "for k in ['A', 'B', 'C']:\n",
        "  tokenized_test[k] = {\"X\":[], \"Y\":[]}\n",
        "\n",
        "for cefr, src, ref in zip(cefr_test, X_test, Y_test):\n",
        "  band = cefr[0]\n",
        "  tokenized_test[band][\"X\"].append(src)\n",
        "  tokenized_test[band][\"Y\"].append(ref)\n",
        "\n",
        "print(tokenizer.decode(tokenized_test['A']['X'][0]))\n",
        "print(tokenizer.decode(tokenized_test['A']['Y'][0]))\n",
        "\n",
        "# To get all X_test and Y_test for A* cefrs\n",
        "# X_test_A = tokenized_test['A']['X']\n",
        "# Y_test_A = tokenized_test['A']['Y']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUj-Z2sU1ltv",
        "outputId": "c4836eef-a003-4e1e-c4b1-eafbae00b0ef"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is Kamaldeen I'm 31 years old I'm married and I don't have children. I moved with my family from Jordan to Saudi Arabia in 1995. I began studying when I came to Saudi Arabia in third grade. I rose up in Al Qassim region and I lived the best time in my life in Al Qassim. I have big family contains from my father and mother and I have seven brothers and three sisters. I studied in Al Qassim University and my major Microbiology. I graduated from my college in 2001 after that I got job in ministry of health and I have been working there since 2010. I like a lot of activities such as travelling, readillng, play soccer and watch movies. </s>\n",
            "My name is Kamaldeen. I'm 31 years old. I'm married and I don't have children. I moved with my family from Jordan to Saudi Arabia in 1995. I began studying when I came to Saudi Arabia in third grade. I grew up in Al Qassim region and I had the best time of my life in Al Qassim. I have a big family <unk> from my father and mother and I have seven brothers and three sisters. I studied at Al Qassim University and my major was Microbiology. I graduated from my college in 2001. After that, I got a job at the Ministry of Health and I have been working there since 2010. I like a lot of activities, such as travelling, reading, playing soccer and watching movies. </s>\n",
            "122\n",
            "112\n",
            "66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_test = {}\n",
        "for k in ['A', 'B', 'C']:\n",
        "  decoded_test[k] = {\"X\":[], \"Y\":[]}\n",
        "\n",
        "for cefr, src, ref in zip(cefr_test, X_test, Y_test):\n",
        "  band = cefr[0]\n",
        "  decoded_test[band][\"X\"].append(tokenizer.decode(src))\n",
        "  decoded_test[band][\"Y\"].append(tokenizer.decode(ref))\n",
        "\n",
        "print(decoded_test['A']['X'][0])\n",
        "print(decoded_test['A']['Y'][0])\n",
        "\n",
        "# To get all X_test and Y_test for A* cefrs\n",
        "# X_test_decoded_A = tokenized_test['A']['X']\n",
        "# Y_test_decoded_A = tokenized_test['A']['Y']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FMQ3rd-2saA",
        "outputId": "c8341f64-3b3b-41c2-e888-997365543f3f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In modern growing world, The basic needs of general public are increasing day by day and their expectations on quality of life took a new dimensions. When we were in our childhood, we used to deal with public transport for going one place to another because that was only a cheaper option available. But today, Things are getting change and technology marked a significant role in our life, Automobile segment increased its vertical and having a car becomes a need from luxory. We can see nowadays, more and more people would prefer to use their own car instead of buses or taxis because they feel comfortable and mobile in it. A car can help him to go anywhere and anytime in much less time as compared to public transport. I believe, as soon as Automobile sector develop new technology in cars people will more keen to use their own vehicle instead or public transport.</s>\n",
            "In the modern <unk> world, the basic needs of the general public are increasing day by day and their expectations of quality of life have taken on new dimensions. When we were children, we used to use public transport for going from one place to another because that was the only cheap option available. But today, things are changing and technology plays a significant role in our lives. The automobile industry increased its <unk> and having a car has become a necessity rather than a luxury. We can see nowadays, more and more people would prefer to use their own car instead of buses or taxis because they feel comfortable and mobile in it. A car can help a person to go anywhere and at any time in much less time compared to public transport. I believe that as soon as the automobile industry develops new technology in cars, people will be more keen to use their own vehicle instead of public transport.</s>\n",
            "dict_keys(['A', 'B', 'C'])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}