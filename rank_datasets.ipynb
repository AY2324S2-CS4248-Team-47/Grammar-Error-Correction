{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Usage\n",
        "\n",
        "input : csv file with rows of [X, Y1, Y2, .. Yn] where\n",
        "  X = erroneous text\n",
        "  Yi = corrected text\n",
        "\n",
        "output: csv file with header [X, rank1, rank2, ..rankn] where Y is ordered by rank"
      ],
      "metadata": {
        "id": "h8jt9rrdd7k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "fccCT3NveyRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from bert_score import BERTScorer\n",
        "import editdistance"
      ],
      "metadata": {
        "id": "YvJKasESuaOK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert(src, target): # returns negated f1 score, so that sent with higher f1 is ranked higher\n",
        "  scorer = BERTScorer(model_type='bert-base-uncased', lang='en')\n",
        "  _, _, F1 = scorer.score([src], [target])\n",
        "\n",
        "  return -1 * F1"
      ],
      "metadata": {
        "id": "YdVzQ8nHgQUh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use Levenshtein distance\n",
        "def get_levenshtein(src, target): # low edit distance ranked higher\n",
        "  return editdistance.eval(src, target)\n",
        "\n",
        "# TODO implement other notions of edit distance? e.g tokenize and compare tokens/contiguous index of edits count as 1 edit?"
      ],
      "metadata": {
        "id": "2w4nDycqgYJl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_csv = 'sample_translation.csv'\n",
        "output_csv = 'ranked_translations.csv'\n",
        "scoring_options = [get_bert, get_levenshtein]\n",
        "scoring_fn = scoring_options[1]"
      ],
      "metadata": {
        "id": "1SA67SLKfmvT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_dataset(input_file, output_file, scoring_fn):\n",
        "  with open(input_file, 'r') as in_file:\n",
        "    reader = csv.reader(in_file)\n",
        "    rows = list(reader)\n",
        "    y_values = len(rows[0]) - 1\n",
        "\n",
        "    ranked_rows = []\n",
        "    for row in rows:\n",
        "      src = row[0]\n",
        "      translations = row[1:]\n",
        "\n",
        "      ranked_row = [src]\n",
        "      ranked_translations = sorted(translations, key=lambda y: scoring_fn(src, y))\n",
        "      ranked_row.extend(ranked_translations)\n",
        "\n",
        "      ranked_rows.append(ranked_row)\n",
        "\n",
        "    with open(output_file, 'w', newline='') as out_file:\n",
        "      writer = csv.writer(out_file)\n",
        "      # write header\n",
        "      header = ['src'] +[f'rank{i}' for i in range(1, y_values+1)]\n",
        "      writer.writerow(header)\n",
        "      writer.writerows(ranked_rows)\n"
      ],
      "metadata": {
        "id": "aAzBQ1bRgBsV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank_dataset(input_csv, output_csv, scoring_fn)"
      ],
      "metadata": {
        "id": "Y3oYLLkRlv8f"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}